{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First PySpark data program\n",
    "\n",
    "## Prerequisite\n",
    "\n",
    "Make sure that PySpark is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.3.2\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 20\n",
      "Branch HEAD\n",
      "Compiled by user liangchi on 2023-02-10T19:57:40Z\n",
      "Revision 5103e00c4ce5fcc4264ca9c4df12295d42557af6\n",
      "Url https://github.com/apache/spark\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!pyspark --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REPL\n",
    "\n",
    "### PySpark REPL\n",
    "\n",
    "The `pyspark` program provides quick and easy access to a Python REPL with PySpark preconfigured. \n",
    "\n",
    "The `Spark context` is then available as `sc` and the `Spark session` is available as `spark`. Spark context is your entry point to Spark, a liaison between your Python REPL and the Spark cluster. Spark session wraps the Spark context and provides you functionalities to interact with the Spark SQL API, which includes the data frame structure.\n",
    "\n",
    "```bash\n",
    "$ pyspark\n",
    "Python 3.9.12 (main, Apr  5 2022, 01:52:34)\n",
    "[Clang 12.0.0 ] :: Anaconda, Inc. on darwin\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "23/04/15 12:03:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.3.2\n",
    "      /_/\n",
    "\n",
    "Using Python version 3.9.12 (main, Apr  5 2022 01:52:34)\n",
    "Spark context Web UI available at http://xiaolis-air:4040\n",
    "Spark context available as 'sc' (master = local[*], app id = local-1681527792485).\n",
    "SparkSession available as 'spark'.\n",
    ">>> spark\n",
    "<pyspark.sql.session.SparkSession object at 0x11607f0d0>\n",
    ">>> sc\n",
    "<SparkContext master=local[*] appName=PySparkShell>\n",
    ">>> spark.sparkContext\n",
    "<SparkContext master=local[*] appName=PySparkShell>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal REPL with Your Configured Spark\n",
    "\n",
    "You can configure the spark session using the builder pattern. When using REPL, do not start the REPL with `pyspark`, but open a normal `python3` REPL, import `SparkSession` , build and configure a spark context as you want.\n",
    "\n",
    "```bash\n",
    "$ python\n",
    ">>> from pyspark.sql import SparkSession\n",
    ">>> spark = (SparkSession.builder.appName(\"Analyzing the vocabulary of Pride and Prejudice\").getOrCreate()\n",
    "```\n",
    "\n",
    "Check out the configured spark context you just built:\n",
    "\n",
    "```REPL\n",
    ">>> spark\n",
    "<pyspark.sql.session.SparkSession object at 0x11772d4f0>\n",
    ">>> spark.sparkContext\n",
    "<SparkContext master=local[*] appName=Analyzing the vocabulary of Pride and Prejudice>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log levels\n",
    "\n",
    "Set log level as follows:\n",
    "\n",
    "```\n",
    "spark.sparkContext.setLogLevel(\"<KEYWORD>\")\n",
    "```\n",
    "Here are possible `KEYWORD`s listed in ascending order of chattiness, each includes the logs of its above levels.\n",
    "\n",
    "- `OFF`: no logging\n",
    "- `FATAL`: fatal errors that will crash your Spark cluster\n",
    "- `ERROR`: fatal and recoverable errors\n",
    "- `WARN`: warnings and errors. This is the default of `pyspark` shell.\n",
    "- `INFO`: runtime information such as repartitioning and data recovery, and everything above. This is the default of non-interactive PySpark program.\n",
    "- `DEBUG`: debug information of your jobs and everything above\n",
    "- `TRACE`: trace your jobs (more verbose debug logs) and everything above\n",
    "- `ALL`: everything PySpark can spit.\n",
    "\n",
    "The `pyspark` shell defaults to `WARN` and non-interactive PySpark programs default to `INFO`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our First Data Preparation Program\n",
    "\n",
    "### Overview\n",
    "\n",
    "We want to find the most used words in Pride and Prejudice. Here are the steps we want to take:\n",
    "1. `Read` input data (assuming a plain text file)\n",
    "2. `Token`ize each word\n",
    "3. `Clean` up: \n",
    "   1. Remove puncuations and non-word tokens\n",
    "   2. Lowercase each word\n",
    "4. `Count` the frequency of each word\n",
    "5. `Answer` return the top 10 (or 20, 50, 100)\n",
    "\n",
    "### Ingest\n",
    "\n",
    "#### Read Data into a Data Frame\n",
    "\n",
    "Data structures:\n",
    "- RDD (Resilient distributed dataset): a distributed collection of objects (or rows). Use regular Python functions to manipulate them.\n",
    "- Dataframe (DF): a stricter version of the RDD, can be seen conceptually as a table. This is the dominant data structure. Operate on columns instead of records.\n",
    "\n",
    "We use `DataFrameReader` object to read data into a data frame. You can access the `DataFrameReader` through `spark.read`, let's print its content to see what's there\n",
    "\n",
    "```\n",
    ">>> spark.read\n",
    "<pyspark.sql.readwriter.DataFrameReader object at 0x102eb0f70>\n",
    ">>> dir(spark.read)\n",
    "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_df', '_jreader', '_set_opts', '_spark', 'csv', 'format', 'jdbc', 'json', 'load', 'option', 'options', 'orc', 'parquet', 'schema', 'table', 'text']\n",
    "```\n",
    "\n",
    "It accomodates different data formats, csv, text, JSON, ORC(optimized row columnar), Parquet, etc. By default PySpark uses Parquet in reading and writing files. Under the hood, `spark.read.csv()` will map to `spark.read.format('csv').load()`.\n",
    "\n",
    "(Since it's hard to note down everything when using REPL, I'm changing to use Jupyter notebook in this note from here on.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pyspark.sql' from '/Users/xiaolishen/opt/miniconda3/lib/python3.9/site-packages/pyspark/sql/__init__.py'>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the SparkSession object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/15 14:49:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/04/15 14:49:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://xiaolis-air:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Analyzing the vocabulary of Pride and Prejudice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x120653ac0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Analyzing the vocabulary of Pride and Prejudice\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the text file\n",
    "\n",
    "We first load the text into a DataFrame object using the `spark.read.text()`. Printing the object gives us the schema (column name and its data type). PySpark data frames also provides a `printSchema()` method to print schema in a tree form to give you more information on the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[value: string]\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "StructType(List(StructField(value,StringType,true)))\n",
      "[('value', 'string')]\n"
     ]
    }
   ],
   "source": [
    "book = spark.read.text(\"../data/gutenberg_books/1342-0.txt\")\n",
    "print(book)\n",
    "\n",
    "# to get more information on the schema\n",
    "book.printSchema()\n",
    "\n",
    "# or access the schema, or the dtypes directly\n",
    "print(book.schema)\n",
    "print(book.dtypes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data will be distributed across multiple nodes, each one a segment of the records. However, when working with data frames, we only need to worry about the `logical schema`, which is the same as if the data is all on a single node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore\n",
    "\n",
    "Use the DataFrame object's `show()` method to explore the data content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+\n",
      "|                                             value|\n",
      "+--------------------------------------------------+\n",
      "|The Project Gutenberg EBook of Pride and Prejud...|\n",
      "|                                                  |\n",
      "|This eBook is for the use of anyone anywhere at...|\n",
      "|almost no restrictions whatsoever.  You may cop...|\n",
      "|re-use it under the terms of the Project Gutenb...|\n",
      "|    with this eBook or online at www.gutenberg.org|\n",
      "|                                                  |\n",
      "|                                                  |\n",
      "|                        Title: Pride and Prejudice|\n",
      "|                                                  |\n",
      "+--------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book.show(10, truncate=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more complex data we'd normally have a lot of columns. We can use `select()` method to select a column in a data frame. PySpark provides multiple ways to access a column:\n",
    "- dot notation `book.value` - as long as the column name doesn't contain funny characters such as `$!@#`\n",
    "- double quote `book[\"value\"]`\n",
    "- `col` function, where you need to first import it as `from pyspark.sql.functions import col`\n",
    "- give the column name as a string and PySpark will infer it to be a column `book.select(\"value\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|The Project Guten...|\n",
      "|                    |\n",
      "|This eBook is for...|\n",
      "|almost no restric...|\n",
      "|re-use it under t...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book.select(book.value).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform\n",
    "\n",
    "#### Tokenization\n",
    "\n",
    "##### Transform sentences to lists of words\n",
    "\n",
    "To transform the data in each row from a long sentence to a list of words, we use the\n",
    "- `split()` function from `pyspark.sql.functions` to split sentences into words\n",
    "- `select()` method to select data\n",
    "- `alias()` method to rename transformed columns. By default the `split()` function gives us a very unintuitive name (`split(value, ,-1)`) and we want something better.\n",
    "\n",
    "The `split()` function refers to the JVM equivalent, which makes it so fast. As a trade off, you'd have to use JVM-base regular expressions (where we passed the `\" \"` as the delimiter) instead of Python regular expressions.\n",
    "\n",
    "Note that the `select` and the transformation happening inside doesn't change the orginal data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                line|\n",
      "+--------------------+\n",
      "|[The, Project, Gu...|\n",
      "|                  []|\n",
      "|[This, eBook, is,...|\n",
      "|[almost, no, rest...|\n",
      "|[re-use, it, unde...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|The Project Guten...|\n",
      "|                    |\n",
      "|This eBook is for...|\n",
      "|almost no restric...|\n",
      "|re-use it under t...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "lines = book.select(\n",
    "    split(book.value, \" \").\n",
    "    alias(\"line\")\n",
    ")\n",
    "lines.show(5)\n",
    "book.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explode rows of lists to rows of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      word|\n",
      "+----------+\n",
      "|       The|\n",
      "|   Project|\n",
      "| Gutenberg|\n",
      "|     EBook|\n",
      "|        of|\n",
      "|     Pride|\n",
      "|       and|\n",
      "|Prejudice,|\n",
      "|        by|\n",
      "|      Jane|\n",
      "|    Austen|\n",
      "|          |\n",
      "|      This|\n",
      "|     eBook|\n",
      "|        is|\n",
      "|       for|\n",
      "|       the|\n",
      "|       use|\n",
      "|        of|\n",
      "|    anyone|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "words = lines.select(\n",
    "    explode(col(\"line\")).\n",
    "    alias(\"word\")\n",
    ")\n",
    "words.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowercase all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|word_lower|\n",
      "+----------+\n",
      "|       the|\n",
      "|   project|\n",
      "| gutenberg|\n",
      "|     ebook|\n",
      "|        of|\n",
      "|     pride|\n",
      "|       and|\n",
      "|prejudice,|\n",
      "|        by|\n",
      "|      jane|\n",
      "|    austen|\n",
      "|          |\n",
      "|      this|\n",
      "|     ebook|\n",
      "|        is|\n",
      "|       for|\n",
      "|       the|\n",
      "|       use|\n",
      "|        of|\n",
      "|    anyone|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower\n",
    "words_lower = words.select(\n",
    "    lower(col(\"word\")).\n",
    "    alias(\"word_lower\")\n",
    ")\n",
    "words_lower.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove non-words with Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|      the|\n",
      "|  project|\n",
      "|gutenberg|\n",
      "|    ebook|\n",
      "|       of|\n",
      "|    pride|\n",
      "|      and|\n",
      "|prejudice|\n",
      "|       by|\n",
      "|     jane|\n",
      "|   austen|\n",
      "|         |\n",
      "|     this|\n",
      "|    ebook|\n",
      "|       is|\n",
      "|      for|\n",
      "|      the|\n",
      "|      use|\n",
      "|       of|\n",
      "|   anyone|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "words_clean = words_lower.select(\n",
    "    regexp_extract(\n",
    "        col(\"word_lower\"),\n",
    "        \"[a-z]+\",\n",
    "        0\n",
    "    ).alias(\"word\")\n",
    ")\n",
    "words_clean.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Filtering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To filter out the empty rows, use either `.filter()` of its alias `where()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|      the|\n",
      "|  project|\n",
      "|gutenberg|\n",
      "|    ebook|\n",
      "|       of|\n",
      "|    pride|\n",
      "|      and|\n",
      "|prejudice|\n",
      "|       by|\n",
      "|     jane|\n",
      "|   austen|\n",
      "|     this|\n",
      "|    ebook|\n",
      "|       is|\n",
      "|      for|\n",
      "|      the|\n",
      "|      use|\n",
      "|       of|\n",
      "|   anyone|\n",
      "| anywhere|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_nonull = words_clean.filter(col(\"word\") != \"\")\n",
    "words_nonull.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercices\n",
    "\n",
    "### Exercise 2.2\n",
    "\n",
    "Given the following dataframe, count the number of columns that aren't strings. `createDataFrame()` lets you create a data frame from multiple sources.\n",
    "\n",
    "`SparkSession.createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- one: string (nullable = true)\n",
      " |-- two: string (nullable = true)\n",
      " |-- three: long (nullable = true)\n",
      "\n",
      "+-----+-----------+-----------+\n",
      "|  one|        two|      three|\n",
      "+-----+-----------+-----------+\n",
      "| test|  more test|10000000000|\n",
      "|test2|more test 2|       1234|\n",
      "+-----+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# createDataFrame(rows[cols[]], column_names[])\n",
    "exo2_2_df = spark.createDataFrame(\n",
    "    [\n",
    "        [\"test\", \"more test\", 10_000_000_000],\n",
    "        [\"test2\", \"more test 2\", 1234]\n",
    "    ], \n",
    "    [\"one\", \"two\", \"three\"]\n",
    ")\n",
    "exo2_2_df.printSchema()\n",
    "exo2_2_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now count the number of columns that aren't strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('one', 'string'), ('two', 'string'), ('three', 'bigint')]\n",
      "[('three', 'bigint')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(exo2_2_df.dtypes)\n",
    "non_string_cols = [\n",
    "    column \n",
    "        for column in exo2_2_df.dtypes \n",
    "        if not column[1].startswith('string')\n",
    "]\n",
    "print(non_string_cols)\n",
    "len(non_string_cols)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3\n",
    "\n",
    "Rewrite the code to remove `withColumnRenamed` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|number_of_char|\n",
      "+--------------+\n",
      "|            66|\n",
      "|             0|\n",
      "|            64|\n",
      "|            68|\n",
      "|            67|\n",
      "+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, length\n",
    "# The `length` function returns the number of characters in a string column.\n",
    "exo2_3_df = (\n",
    "    spark.read.text(\"../data/gutenberg_books/1342-0.txt\") \n",
    "    .select(length(col(\"value\"))) \n",
    "    .withColumnRenamed(\"length(value)\", \n",
    "                       \"number_of_char\")\n",
    ")\n",
    "exo2_3_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|number_of_char|\n",
      "+--------------+\n",
      "|            66|\n",
      "|             0|\n",
      "|            64|\n",
      "|            68|\n",
      "|            67|\n",
      "+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exo2_3_df = spark.read.text(\n",
    "    \"../data/gutenberg_books/1342-0.txt\"\n",
    ").select(\n",
    "    length(\n",
    "        \"value\"\n",
    "    ).alias(\"number_of_char\")\n",
    ")\n",
    "\n",
    "exo2_3_df.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.4\n",
    "What is the problem, and how can you solve it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value1: long (nullable = true)\n",
      " |-- value2: long (nullable = true)\n",
      "\n",
      "Column 'key' does not exist. Did you mean one of the following? [maximum_value];\n",
      "'Project ['key, 'max_value]\n",
      "+- Project [greatest(value1#367L, value2#368L) AS maximum_value#372L]\n",
      "   +- LogicalRDD [key#366, value1#367L, value2#368L], false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, greatest\n",
    "exo2_4_df = spark.createDataFrame(\n",
    "    [[\"value\", 10_000, 20_000]], [\"key\", \"value1\", \"value2\"]\n",
    ")\n",
    "exo2_4_df.printSchema()\n",
    "# root\n",
    "#  |-- key: string (containsNull = true)\n",
    "#  |-- value1: long (containsNull = true)\n",
    "#  |-- value2: long (containsNull = true)\n",
    "# `greatest` will return the greatest value of the list of column names,\n",
    "# skipping null value\n",
    "# The following statement will return an error\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "try:\n",
    "    exo2_4_mod = exo2_4_df.select(\n",
    "greatest(col(\"value1\"), col(\"value2\")).alias(\"maximum_value\") ).select(\"key\", \"max_value\")\n",
    "except AnalysisException as err:\n",
    "    print(err)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code tries to select \"key\" column from the result of `greatest` , which doesn't exist there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|  key|maximum_value|\n",
      "+-----+-------------+\n",
      "|value|        20000|\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exo2_4_mod = exo2_4_df.select(\n",
    "    \"key\",\n",
    "    greatest(\n",
    "        col(\"value1\"), col(\"value2\")\n",
    "    ).alias(\"maximum_value\")\n",
    ")\n",
    "\n",
    "exo2_4_mod.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.5\n",
    "\n",
    "Take our words_nonull data frame, \n",
    "1. Remove all of the occurrences of the word is.\n",
    "2. (Challenge) Using the length function, keep only the words with more than three\n",
    "characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|      the|\n",
      "|  project|\n",
      "|gutenberg|\n",
      "|    ebook|\n",
      "|       of|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|      the|\n",
      "|  project|\n",
      "|gutenberg|\n",
      "|    ebook|\n",
      "|    pride|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, lower, regexp_extract, length\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "book = spark.read.text(\"../data/gutenberg_books/1342-0.txt\")\n",
    "\n",
    "words_nonull = book.select(\n",
    "        split(book.value, \" \").alias(\"line\")\n",
    "    ).select(\n",
    "        explode(\"line\").alias(\"word\")\n",
    "    ).select(\n",
    "        lower(\"word\").alias(\"word_lower\")\n",
    "    ).select(\n",
    "        regexp_extract(\"word_lower\", \"[a-z]+\", 0).\n",
    "            alias(\"word\")\n",
    "    )\n",
    "\n",
    "no_is_or_null = words_nonull.where(\n",
    "        ~col(\"word\").isin([\"\", \"is\"])\n",
    "    ).show(5)\n",
    "\n",
    "longer_than_three_chars = words_nonull.where(\n",
    "    length(col(\"word\")) >= 3\n",
    ").show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.6\n",
    "\n",
    "Remove the words \"is, not, the, if\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|  project|\n",
      "|gutenberg|\n",
      "|    ebook|\n",
      "|    pride|\n",
      "|      and|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned = words_nonull.where(\n",
    "    (~col(\"word\").isin([\"\", \"is\", \"not\", \"the\", \"if\"])) & (length(col(\"word\")) >= 3)\n",
    ").show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.7\n",
    "\n",
    "Diagnose the problem in the try block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'select'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m     book \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39mtext(\u001b[39m\"\u001b[39m\u001b[39m../data/gutenberg_books/1342-0.txt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m     book \u001b[39m=\u001b[39m book\u001b[39m.\u001b[39mprintSchema()\n\u001b[0;32m----> 5\u001b[0m     lines \u001b[39m=\u001b[39m book\u001b[39m.\u001b[39;49mselect(split(book\u001b[39m.\u001b[39mvalue, \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mline\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m      6\u001b[0m     words \u001b[39m=\u001b[39m lines\u001b[39m.\u001b[39mselect(explode(col(\u001b[39m\"\u001b[39m\u001b[39mline\u001b[39m\u001b[39m\"\u001b[39m))\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mword\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m      7\u001b[0m \u001b[39mexcept\u001b[39;00m AnalysisException \u001b[39mas\u001b[39;00m err:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'select'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, split\n",
    "try:\n",
    "    book = spark.read.text(\"../data/gutenberg_books/1342-0.txt\")\n",
    "    book = book.printSchema()\n",
    "    lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
    "    words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    "except AnalysisException as err:\n",
    "    print(err)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`printSchema()` returns `None`. In line 4 you assiged `book.printSchema()` to `book`, making the variable `book` now `None`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
