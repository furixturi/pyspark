{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First PySpark data program\n",
    "\n",
    "## Prerequisite\n",
    "\n",
    "Make sure that PySpark is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.3.2\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 20\n",
      "Branch HEAD\n",
      "Compiled by user liangchi on 2023-02-10T19:57:40Z\n",
      "Revision 5103e00c4ce5fcc4264ca9c4df12295d42557af6\n",
      "Url https://github.com/apache/spark\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!pyspark --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REPL\n",
    "\n",
    "### PySpark REPL\n",
    "\n",
    "The `pyspark` program provides quick and easy access to a Python REPL with PySpark preconfigured. \n",
    "\n",
    "The `Spark context` is then available as `sc` and the `Spark session` is available as `spark`. Spark context is your entry point to Spark, a liaison between your Python REPL and the Spark cluster. Spark session wraps the Spark context and provides you functionalities to interact with the Spark SQL API, which includes the data frame structure.\n",
    "\n",
    "```bash\n",
    "$ pyspark\n",
    "Python 3.9.12 (main, Apr  5 2022, 01:52:34)\n",
    "[Clang 12.0.0 ] :: Anaconda, Inc. on darwin\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "23/04/15 12:03:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.3.2\n",
    "      /_/\n",
    "\n",
    "Using Python version 3.9.12 (main, Apr  5 2022 01:52:34)\n",
    "Spark context Web UI available at http://xiaolis-air:4040\n",
    "Spark context available as 'sc' (master = local[*], app id = local-1681527792485).\n",
    "SparkSession available as 'spark'.\n",
    ">>> spark\n",
    "<pyspark.sql.session.SparkSession object at 0x11607f0d0>\n",
    ">>> sc\n",
    "<SparkContext master=local[*] appName=PySparkShell>\n",
    ">>> spark.sparkContext\n",
    "<SparkContext master=local[*] appName=PySparkShell>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal REPL with Your Configured Spark\n",
    "\n",
    "You can configure the spark session using the builder pattern. When using REPL, do not start the REPL with `pyspark`, but open a normal `python3` REPL, import `SparkSession` , build and configure a spark context as you want.\n",
    "\n",
    "```bash\n",
    "$ python\n",
    ">>> from pyspark.sql import SparkSession\n",
    ">>> spark = (SparkSession.builder.appName(\"Analyzing the vocabulary of Pride and Prejudice\").getOrCreate()\n",
    "```\n",
    "\n",
    "Check out the configured spark context you just built:\n",
    "\n",
    "```REPL\n",
    ">>> spark\n",
    "<pyspark.sql.session.SparkSession object at 0x11772d4f0>\n",
    ">>> spark.sparkContext\n",
    "<SparkContext master=local[*] appName=Analyzing the vocabulary of Pride and Prejudice>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log levels\n",
    "\n",
    "Set log level as follows:\n",
    "\n",
    "```\n",
    "spark.sparkContext.setLogLevel(\"<KEYWORD>\")\n",
    "```\n",
    "Here are possible `KEYWORD`s listed in ascending order of chattiness, each includes the logs of its above levels.\n",
    "\n",
    "- `OFF`: no logging\n",
    "- `FATAL`: fatal errors that will crash your Spark cluster\n",
    "- `ERROR`: fatal and recoverable errors\n",
    "- `WARN`: warnings and errors. This is the default of `pyspark` shell.\n",
    "- `INFO`: runtime information such as repartitioning and data recovery, and everything above. This is the default of non-interactive PySpark program.\n",
    "- `DEBUG`: debug information of your jobs and everything above\n",
    "- `TRACE`: trace your jobs (more verbose debug logs) and everything above\n",
    "- `ALL`: everything PySpark can spit.\n",
    "\n",
    "The `pyspark` shell defaults to `WARN` and non-interactive PySpark programs default to `INFO`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our First Data Preparation Program\n",
    "\n",
    "### Overview\n",
    "\n",
    "We want to find the most used words in Pride and Prejudice. Here are the steps we want to take:\n",
    "1. `Read` input data (assuming a plain text file)\n",
    "2. `Token`ize each word\n",
    "3. `Clean` up: \n",
    "   1. Remove puncuations and non-word tokens\n",
    "   2. Lowercase each word\n",
    "4. `Count` the frequency of each word\n",
    "5. `Answer` return the top 10 (or 20, 50, 100)\n",
    "\n",
    "### Ingest\n",
    "\n",
    "#### Read Data into a Data Frame\n",
    "\n",
    "Data structures:\n",
    "- RDD (Resilient distributed dataset): a distributed collection of objects (or rows). Use regular Python functions to manipulate them.\n",
    "- Dataframe (DF): a stricter version of the RDD, can be seen conceptually as a table. This is the dominant data structure. Operate on columns instead of records.\n",
    "\n",
    "We use `DataFrameReader` object to read data into a data frame. You can access the `DataFrameReader` through `spark.read`, let's print its content to see what's there\n",
    "\n",
    "```\n",
    ">>> spark.read\n",
    "<pyspark.sql.readwriter.DataFrameReader object at 0x102eb0f70>\n",
    ">>> dir(spark.read)\n",
    "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_df', '_jreader', '_set_opts', '_spark', 'csv', 'format', 'jdbc', 'json', 'load', 'option', 'options', 'orc', 'parquet', 'schema', 'table', 'text']\n",
    "```\n",
    "\n",
    "### Explore\n",
    "\n",
    "### Transform\n",
    "\n",
    "### Filtering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
