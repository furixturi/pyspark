{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First PySpark Program Continued"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Our program is planned as follows:\n",
    "We want to find the most used words in Pride and Prejudice. Here are the steps we want to take:\n",
    "1. `Read` input data (assuming a plain text file)\n",
    "2. `Token`ize each word\n",
    "3. `Clean` up: \n",
    "   1. Remove puncuations and non-word tokens\n",
    "   2. Lowercase each word\n",
    "4. `Count` the frequency of each word\n",
    "5. `Answer` return the top 10 (or 20, 50, 100)\n",
    "\n",
    "In chapter 2 we've done 1~3. Now we want to do 4~5,submit our first PySpark program, and also organize our program into multiple Python files."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group, Order and Aggregate the Records"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group\n",
    "\n",
    "To group a data frame's records into groups, we use data frame's `groupby()` method and pass the columns we want to group as the parameter. \n",
    "\n",
    "This method returns a `GroupData` object on which we can apply aggregation functions such as `count()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The usual initialization\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, explode, lower, regexp_extract, length, col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Analyzing the vocabulary of Pride and Prejudice\").getOrCreate()\n",
    "\n",
    "# load and preprocess data\n",
    "book = spark.read.text(\"../data/gutenberg_books/1342-0.txt\")\n",
    "words_nonull = book.select(\n",
    "        split(book.value, \" \").alias(\"line\")\n",
    "    ).select(\n",
    "        explode(\"line\").alias(\"word\")\n",
    "    ).select(\n",
    "        lower(\"word\").alias(\"word_lower\")\n",
    "    ).select(\n",
    "        regexp_extract(\"word_lower\", \"[a-z]+\", 0).\n",
    "            alias(\"word\")\n",
    "    ).where(\n",
    "        length(col(\"word\")) > 0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.group.GroupedData object at 0x10a0fcd00>\n"
     ]
    }
   ],
   "source": [
    "# group\n",
    "groups = words_nonull.groupBy(col(\"word\"))\n",
    "print(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|  word|count|\n",
      "+------+-----+\n",
      "|online|    4|\n",
      "|  some|  209|\n",
      "| still|   72|\n",
      "|   few|   72|\n",
      "|  hope|  122|\n",
      "+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = words_nonull.groupBy(\n",
    "        col(\"word\")\n",
    "    ).count()\n",
    "results.show(5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order\n",
    "\n",
    "Use `orderBy` to order the results by a column. You can pass multiple columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the| 4496|\n",
      "|  to| 4235|\n",
      "|  of| 3719|\n",
      "| and| 3602|\n",
      "| her| 2223|\n",
      "+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.orderBy(\"count\", ascending=False).show(5)\n",
    "# alternativly\n",
    "ordered_results = results.orderBy(col(\"count\").desc())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write data from a data frame\n",
    "\n",
    "Just as we use `read()` and the `SparkReader` to read data in Spark, we use `write()` and the `SparkWriter` to write data from a data frame to disk. Let's do a CSV.\n",
    "\n",
    "Note that Spark will not write to a single CSV file but create a folder with the path we specified and write as many output files as there are partitions and a `_SUCCESS` file. It doesn't guarantee any order unless you do a `orderBy` first. However, if your data is big, it is quite expensive to order them before writing to disk. Furthermore, since it is normally another distributed program that will read the written data, there's generally no point ordering the data at write time).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.write.mode(\"overwrite\").csv(\"./data/simple_count\")\n",
    "ordered_results.write.mode(\"overwrite\").csv(\"./data/ordered_count\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify the desired number of partitions, apply `coalesce()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_results.coalesce(1).write.csv(\"./data/coalesced\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplify your Code\n",
    "\n",
    "### Simplify dependency import\n",
    "\n",
    "Instead of importing each function we need in the `pyspark.sql.functions` module, import the whole module as an object `F`. (Don't import `*` from the package, otherweise your readers cannot tell which functions you use are from the package, which are Python native functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplify with method chaining\n",
    "\n",
    "Each data frame method returns the data frame after the transformation, so you can chain the transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the| 4480|\n",
      "|  to| 4218|\n",
      "|  of| 3711|\n",
      "| and| 3504|\n",
      "| her| 2199|\n",
      "+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = (\n",
    "    spark.read.text(\"../data/gutenberg_books/1342-0.txt\")\n",
    "    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
    "    .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    "    .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
    "    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\n",
    "    .where(F.length(F.col(\"word\")) > 0)\n",
    "    .groupBy(\"word\")\n",
    "    .count()\n",
    ")\n",
    "\n",
    "results.orderBy(\"count\", ascending=False).show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit to PySpark to Run as a Batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is in `./code/word_count_submit.py`. We read all text files in the `../data/gutenberg_books/` folder and find the most frequent words.\n",
    "\n",
    "Under the project folder, run\n",
    "\n",
    "```bash\n",
    "$ spark-submit ./03\\ submit\\ pyspark\\ program/code/word_count_submit.py\n",
    "```\n",
    "\n",
    "We'll see a bunch of INFO and our result. The file is also proerly written to where we specified.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 3.3 Challenge\n",
    "\n",
    "Wrap your program in a function that takes a file name as a param- eter. It should return the number of distinct words.\n",
    "\n",
    "===\n",
    "\n",
    "The `spark-submit` accepts the following options\n",
    "\n",
    "```bash\n",
    "./bin/spark-submit \\\n",
    "  --class <main-class> \\\n",
    "  --master <master-url> \\\n",
    "  --deploy-mode <deploy-mode> \\\n",
    "  --conf <key>=<value> \\\n",
    "  ... # other options\n",
    "  <application-jar> \\\n",
    "  [application-arguments]\n",
    "```\n",
    "\n",
    "What we pass `[[application-arguments]]` simply after our script file. (If we're not running locally, we must first specify the master URL in `--master` option before giving our script file).\n",
    "\n",
    "Then in our script, the parameters will be in the `sys.argv` array, starting from index 1 as index 0 is the script file.\n",
    "\n",
    "We wrote the file as accepting two parameters: the text file path and the top N frequent words to show.\n",
    "\n",
    "```bash\n",
    "$ spark-submit ./03\\ submit\\ pyspark\\ program/code/parameterized.py \\\n",
    "    \"./data/gutenberg_books/11-0.txt\" \\\n",
    "    10                                         \n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.4 \n",
    "\n",
    "Modify the script to return a sample of five words that appear only once in Jane Austenâ€™s Pride and Prejudice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      word|\n",
      "+----------+\n",
      "| arguments|\n",
      "| solemnity|\n",
      "|likelihood|\n",
      "|     parts|\n",
      "|    absurd|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.where(col(\"count\") == 5).select(col(\"word\")).show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.5\n",
    "1. Using the substring function (refer to PySparkâ€™s API or the pyspark shell if needed), return the top five most popular first letters (keep only the first letter of each word).\n",
    "2. Compute the number of words starting with a consonant or a vowel. (Hint: The isin() function might be useful.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|first_letter|count|\n",
      "+------------+-----+\n",
      "|           s|  679|\n",
      "|           c|  612|\n",
      "|           p|  529|\n",
      "|           a|  527|\n",
      "|           d|  485|\n",
      "+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    results.select(\n",
    "        F.substring(\n",
    "            col(\"word\"),1,1\n",
    "        ).alias(\"first_letter\")\n",
    "    ).\n",
    "    groupBy(\"first_letter\")\n",
    "    .count()\n",
    "    .orderBy(col(\"count\"), ascending=False)\n",
    "    .show(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6577 1619 4958\n"
     ]
    }
   ],
   "source": [
    "first_letters = results.select(\n",
    "    F.substring(col(\"word\"), 1, 1).alias(\"first_letter\")\n",
    ").where(col(\"first_letter\").rlike(\"[a-z]\")) # make sure to only keep the words starting with a letter, not '\n",
    "\n",
    "total_word_count = first_letters.count()\n",
    "\n",
    "vowel_word_count = first_letters.where(\n",
    "    col(\"first_letter\").isin([\"a\", \"e\", \"i\", \"o\", \"u\"])\n",
    ").count()\n",
    "\n",
    "consonant_word_count = total_word_count - vowel_word_count\n",
    "\n",
    "print(total_word_count, vowel_word_count, consonant_word_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.6\n",
    "\n",
    "Why doesn't the following work?\n",
    "\n",
    "```\n",
    "my_data_frame.groupby(\"my_column\").count().sum()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using `count()` in aggregation, it called on the `GroupData` object returned by the `groupBy`, which returns two columns: the originally grouped column plus a column named `count`. `sum()` only works on one column, so it cannot be applied to the result of `count()`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
